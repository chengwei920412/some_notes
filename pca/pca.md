# PCA

### 问题

     真实的训练数据总是存在各种各样的问题：

- 比如拿到一个汽车的样本，里面既有以“千米/每小时”度量的最大速度特征，也有“英里/小时”的最大速度特征，显然这两个特征有一个多余。
- 拿到一个数学系的本科生期末考试成绩单，里面有三列，一列是对数学的兴趣程度，一列是复习时间，还有一列是考试成绩。我们知道要学好数学，需要有浓厚的兴趣，所以第二项与第一项强相关，第三项和第二项也是强相关。那是不是可以合并第一项和第二项呢？
-  拿到一个样本，特征非常多，而样例特别少，这样用回归去直接拟合非常困难，容易过度拟合。比如北京的房价：假设房子的特征是（大小、位置、朝向、是否学区房、建造年代、是否二手、层数、所在层数），搞了这么多特征，结果只有不到十个房子的样例。要拟合房子特征->房价的这么多特征，就会造成过度拟合。

下面探讨一种称作主成分分析（PCA）的方法来解决部分上述问题。PCA的思想是将$n$维特征映射到k维$（k<n）$，这$k$维是全新的正交特征。这$k$维特征称为主元，是重新构造出来的$k$维特征，而不是简单地从$n$维特征中去除其余$n-k$维特征。



### 计算过程

1. 计算各个特征的平均值。
2. 求特征协方差矩阵。
3. 求协方差的特征值和特征向量。
4. 将特征值按照从大到小的顺序排序，选择其中最大的$k$个，然后将其对应的$k$个特征向量分别作为列向量组成特征向量矩阵。
5. 将样本点投影到选取的特征向量上。假设样例数为$m$，特征数为$n$，减去均值后的样本矩阵为DataAdjust$(m*n)$，协方差矩阵为$m*n$，选取的$k$个特征向量组成的矩阵为EigenVectors$(n*k)$。那么投影后的数据WiFinalData为

$$
FinalData(m*k)=DataAdjust(m*n)×EigenVectors(n*k)
$$



### 理论基础

##### 最大方差理论

     在信号处理中认为信号具有较大的方差，噪声有较小的方差，信噪比就是信号与噪声的方差比，越大越好。因此我们认为，最好的k维特征是将n维样本点转换为k维后，每一维上的样本方差都很大。

      比如下图有5个样本点：（已经做过预处理，均值为0，特征方差归一）

![](/home/liu/Documents/pca/yuantu.png)

      下面将样本投影到某一维上，这里用一条过原点的直线表示（前处理的过程实质是将原点移到样本点的中心点）。

![](/home/liu/Documents/pca/zhuanhuantu.jpg)

​	假设我们选择两条不同的直线做投影，那么左右两条中哪个好呢？根据我们之前的方差最大化理论，左边的好，因为投影后的样本点之间方差最大。

![](/home/liu/Documents/pca/touying.png)

​	红色点表示样例$x^{(i)}$，蓝色点表示$x^{(i)}$在$u$上的投影，$u$是直线的斜率也是直线的方向向量，而且是单位向量。蓝色点是$x^{(i)}$在$u$上的投影点，离原点的距离是$<x^{(i)},u>$(即$x^{(i)^T}u$或者$u^Tx^{(i)}$)由于这些样本点（样例）的每一维特征均值都为0，因此投影到$u$上的样本点（只有一个到原点的距离值）的均值仍然是0。

​	 回到上面左右图中的左图，我们要求的是最佳的u，使得投影后的样本点方差最大。由于投影后均值为0，因此方差为：
$$
\begin{align}
\frac {1}{m}\sum_{i=1}^{m}(x^{(i)^T}u)^2 &= \frac{1}{m}\sum_{m=1}^{m}u^Tx^{{(i)}}x^{{(i)^T}}u\\
&= u^T(\frac{1}{m}\sum_{i=1}^{m}x^{(i)}x^{(i)^T})u.
\end{align}
$$

​	中间那部分很熟悉啊，不就是样本特征的协方差矩阵么，（$x^{{(i)}}$的均值为0，一般协方差矩阵都除以$m-1$，这里用$m$）。

​	用$λ$来表示$\frac {1}{m}\sum_{i=1}^{m}(x^{(i)^T}u)^2$，$Σ$表示$ \frac{1}{m}\sum_{i=1}^{m}x^{(i)}x^{(i)^T} $，那么上式写作
$$
λ=u^T\sum u
$$
​	由于$u$是单位向量，即$u^Tu=1$，上式两边都左乘$u$得，$uλ=λu=uu^T\sum u=\sum u$

​	即$\sum u = λu$

​	We got it！$\lambda$就是$\sum$的特征值，$u$是特征向量。最佳的投影只显示特征值$\lambda$最大时对应的特征向量，其次是$\lambda$第二大对应的特征向量，依次类推。

​	因此，我们只需要对协方差矩阵进行特征值分解，得到的前$k$大特征值对应的特征向量就是最佳的$k$维新特征，而且这$k$维新特征是正交的。得到前$k$个$u$以后，样例$x^{(i)}$通过以下变换可以得到新的样本。
$$
y^{(i)}=\left\{
\begin{matrix}
   u_1^Tx^{(i)}  \\
   u_2^Tx^{(i)}   \\
   ...  \\
   u_k^Tx^{(i)} 
  \end{matrix} 
  \right\}
$$
​	其中第$j$维就是$x^{(i)}$在$u_j$上的投影，通过选取最大的$k$个$u$，使得方差较小的特征（如噪声）被丢弃。

 

### 总结与讨论

​	PCA技术的一大好处是对数据进行降维的处理。我们可以对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。

​	PCA技术的一个很大的优点是，它是完全无参数限制的。在PCA的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。 但是，这一点同时也可以看作是缺点。如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高。

​	有时数据的分布并不是满足高斯分布。如下图所示，在非高斯分布的情况下，PCA方法得出的主元可能并不是最优的。在寻找主元时不能将方差作为衡量重要性的标准。要根据数据的分布情况选择合适的描述完全分布的变量，然后根据概率分布式
$$
P(y_1,y_2)=P(y_1)P(y_2)
$$
​	来计算两个向量上数据分布的相关性。等价的，保持主元间的正交假设，寻找的主元同样要使$P(y_1,y_2)=0$。这一类方法被称为独立主元分解(ICA)。在下图中，数据的分布并不满足高斯分布，呈明显的十字星状。 这种情况下，方差最大的方向并不是最优主元方向。

![](/home/liu/Documents/pca/ICA.gif)